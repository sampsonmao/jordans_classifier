{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef78231-7500-46fa-9a49-5e2e2205acd2",
   "metadata": {},
   "source": [
    "# Classifying Air Jordan Sneakers\n",
    "## Sampson Mao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24116880-46b0-4e1b-9115-38f514a9bb26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185a1e68-43b5-4278-b271-67f733f2ed94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b902973-5931-4abe-918e-1cc93cfd7e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6204c0b1-909b-40fb-926f-b6e35bea0374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"C:\\\\Users\\\\Sampson\\\\Desktop\\\\jordans_classifier\\\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadc6dd-47c6-46b5-9ddd-ea347b071f2f",
   "metadata": {},
   "source": [
    "The images appear to be scraped from the web. Upon further inspection, the \"corrupted\" data weere html files. Here, I remove those files while making sure the files that are actually images have the right color mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645fb2a-bb54-4059-9a79-7227a63909b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(DATA_DIR):\n",
    "    for filename in files:\n",
    "        img_path = os.path.join(path, filename)\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            if img.format not in [\"BMP\", \"GIF\", \"JPEG\", \"PNG\"]:\n",
    "                img.convert(\"RGB\").save(img_path)\n",
    "            elif (img.format == \"PNG\") and (img.mode != \"RGBA\"):\n",
    "                img.convert(\"RGBA\").save(img_path.replace(\".jpg\", \".png\"))\n",
    "                os.remove(img_path)\n",
    "        except:\n",
    "            os.renames(img_path, img_path.replace(\"data\", \"corrupted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f3f50-c1ba-4999-857d-6a8bdeb07c76",
   "metadata": {},
   "source": [
    "While we do have a sizable number of images for each kind of shoe, many of the images do not provide much variety. For example, AJ1s seem to have the same images cropped or zoomed differently. Ideally we would want to remove duplicates but as an initial exploration, we will try data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0800a96f-8fe2-4f54-905d-63b65c883dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b99f47-fc7c-4bb5-a909-03d24a72e1da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7504 images belonging to 21 classes.\n",
      "Found 1867 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    subset=\"training\",\n",
    "    target_size=(256, 256),\n",
    "    class_mode=\"sparse\",\n",
    ")\n",
    "val_ds = datagen.flow_from_directory(\n",
    "    DATA_DIR,\n",
    "    subset=\"validation\",\n",
    "    target_size=(256, 256),\n",
    "    class_mode=\"sparse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dcb33-3971-414c-a7d9-ae16e6ac8761",
   "metadata": {},
   "source": [
    "Here I create a basic Sequential model with 2 convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d06185-c8a7-40ed-86d6-c2894fa005f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filters=16, kernel_size=(3, 3), activation=\"relu\", input_shape=(256, 256, 3)\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(21, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15769ff2-efba-4c1c-9b0a-05ec236e1d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7e9de-01a5-46d4-8e15-f10f6ef141b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 120s 500ms/step - loss: 4.4520 - accuracy: 0.0837 - val_loss: 2.8521 - val_accuracy: 0.1002\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 115s 490ms/step - loss: 2.8106 - accuracy: 0.1066 - val_loss: 2.8499 - val_accuracy: 0.0734\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.7446 - accuracy: 0.1305 - val_loss: 2.9064 - val_accuracy: 0.0927\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 126s 536ms/step - loss: 2.7225 - accuracy: 0.1390 - val_loss: 2.9027 - val_accuracy: 0.0889\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 127s 539ms/step - loss: 2.6621 - accuracy: 0.1680 - val_loss: 2.8802 - val_accuracy: 0.1028\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 120s 512ms/step - loss: 2.6286 - accuracy: 0.1810 - val_loss: 2.9029 - val_accuracy: 0.1296\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 113s 479ms/step - loss: 2.5713 - accuracy: 0.1990 - val_loss: 2.9083 - val_accuracy: 0.1425\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.5255 - accuracy: 0.2158 - val_loss: 2.9047 - val_accuracy: 0.1398\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 113s 481ms/step - loss: 2.4787 - accuracy: 0.2416 - val_loss: 2.9775 - val_accuracy: 0.1259\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 118s 503ms/step - loss: 2.4267 - accuracy: 0.2481 - val_loss: 3.0528 - val_accuracy: 0.1419\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 116s 493ms/step - loss: 2.3922 - accuracy: 0.2613 - val_loss: 2.9145 - val_accuracy: 0.1564\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 114s 485ms/step - loss: 2.3586 - accuracy: 0.2696 - val_loss: 3.0025 - val_accuracy: 0.1660\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.3090 - accuracy: 0.2822 - val_loss: 3.0028 - val_accuracy: 0.1516\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.3043 - accuracy: 0.2885 - val_loss: 2.9978 - val_accuracy: 0.1569\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 113s 479ms/step - loss: 2.2506 - accuracy: 0.3080 - val_loss: 3.0021 - val_accuracy: 0.1751\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 114s 482ms/step - loss: 2.2146 - accuracy: 0.3190 - val_loss: 2.9014 - val_accuracy: 0.1655\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 113s 481ms/step - loss: 2.2204 - accuracy: 0.3206 - val_loss: 2.9822 - val_accuracy: 0.1650\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 113s 479ms/step - loss: 2.1941 - accuracy: 0.3262 - val_loss: 2.8340 - val_accuracy: 0.1944\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 112s 475ms/step - loss: 2.1557 - accuracy: 0.3378 - val_loss: 2.9113 - val_accuracy: 0.1816\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.1205 - accuracy: 0.3493 - val_loss: 2.9901 - val_accuracy: 0.2105\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.0938 - accuracy: 0.3622 - val_loss: 2.9281 - val_accuracy: 0.1837\n",
      "Epoch 22/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 2.1009 - accuracy: 0.3575 - val_loss: 2.8918 - val_accuracy: 0.1832\n",
      "Epoch 23/100\n",
      "235/235 [==============================] - 113s 478ms/step - loss: 2.0482 - accuracy: 0.3669 - val_loss: 2.8458 - val_accuracy: 0.2025\n",
      "Epoch 24/100\n",
      "235/235 [==============================] - 114s 484ms/step - loss: 2.0200 - accuracy: 0.3742 - val_loss: 2.9196 - val_accuracy: 0.2084\n",
      "Epoch 25/100\n",
      "235/235 [==============================] - 114s 483ms/step - loss: 1.9941 - accuracy: 0.3886 - val_loss: 2.9459 - val_accuracy: 0.1885\n",
      "Epoch 26/100\n",
      "235/235 [==============================] - 113s 479ms/step - loss: 1.9621 - accuracy: 0.4023 - val_loss: 2.9708 - val_accuracy: 0.2137\n",
      "Epoch 27/100\n",
      "235/235 [==============================] - 114s 485ms/step - loss: 1.9387 - accuracy: 0.4074 - val_loss: 2.9623 - val_accuracy: 0.2030\n",
      "Epoch 28/100\n",
      "235/235 [==============================] - 115s 488ms/step - loss: 1.9385 - accuracy: 0.4114 - val_loss: 2.8926 - val_accuracy: 0.2201\n",
      "Epoch 29/100\n",
      "235/235 [==============================] - 116s 493ms/step - loss: 1.9109 - accuracy: 0.4179 - val_loss: 2.9800 - val_accuracy: 0.2164\n",
      "Epoch 30/100\n",
      "235/235 [==============================] - 116s 495ms/step - loss: 1.9187 - accuracy: 0.4190 - val_loss: 2.9588 - val_accuracy: 0.2116\n",
      "Epoch 31/100\n",
      "235/235 [==============================] - 115s 487ms/step - loss: 1.8730 - accuracy: 0.4282 - val_loss: 2.9687 - val_accuracy: 0.1976\n",
      "Epoch 32/100\n",
      "235/235 [==============================] - 113s 482ms/step - loss: 1.8895 - accuracy: 0.4183 - val_loss: 2.9239 - val_accuracy: 0.2025\n",
      "Epoch 33/100\n",
      "235/235 [==============================] - 114s 483ms/step - loss: 1.8369 - accuracy: 0.4354 - val_loss: 2.8626 - val_accuracy: 0.2309\n",
      "Epoch 34/100\n",
      "235/235 [==============================] - 113s 482ms/step - loss: 1.8108 - accuracy: 0.4463 - val_loss: 2.9768 - val_accuracy: 0.2239\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - 113s 480ms/step - loss: 1.8358 - accuracy: 0.4407 - val_loss: 3.0600 - val_accuracy: 0.2035\n",
      "Epoch 36/100\n",
      "235/235 [==============================] - 126s 535ms/step - loss: 1.7806 - accuracy: 0.4572 - val_loss: 3.0984 - val_accuracy: 0.2051\n",
      "Epoch 37/100\n",
      "235/235 [==============================] - 120s 508ms/step - loss: 1.7624 - accuracy: 0.4576 - val_loss: 3.1358 - val_accuracy: 0.2100\n",
      "Epoch 38/100\n",
      "235/235 [==============================] - 119s 505ms/step - loss: 1.7828 - accuracy: 0.4598 - val_loss: 3.0195 - val_accuracy: 0.2132\n",
      "Epoch 39/100\n",
      "235/235 [==============================] - 117s 500ms/step - loss: 1.7622 - accuracy: 0.4679 - val_loss: 3.0045 - val_accuracy: 0.2244\n",
      "Epoch 40/100\n",
      "235/235 [==============================] - 118s 504ms/step - loss: 1.7072 - accuracy: 0.4872 - val_loss: 3.1289 - val_accuracy: 0.2057\n",
      "Epoch 41/100\n",
      "235/235 [==============================] - 113s 483ms/step - loss: 1.6998 - accuracy: 0.4843 - val_loss: 3.0508 - val_accuracy: 0.2212\n",
      "Epoch 42/100\n",
      "235/235 [==============================] - 116s 493ms/step - loss: 1.7037 - accuracy: 0.4840 - val_loss: 2.9093 - val_accuracy: 0.2437\n",
      "Epoch 43/100\n",
      "235/235 [==============================] - 116s 492ms/step - loss: 1.6741 - accuracy: 0.4869 - val_loss: 3.0208 - val_accuracy: 0.2250\n",
      "Epoch 44/100\n",
      "235/235 [==============================] - 117s 497ms/step - loss: 1.6371 - accuracy: 0.4999 - val_loss: 3.1186 - val_accuracy: 0.2314\n",
      "Epoch 45/100\n",
      "235/235 [==============================] - 120s 508ms/step - loss: 1.6384 - accuracy: 0.4973 - val_loss: 3.0710 - val_accuracy: 0.2373\n",
      "Epoch 46/100\n",
      "235/235 [==============================] - 118s 500ms/step - loss: 1.6466 - accuracy: 0.5020 - val_loss: 3.0781 - val_accuracy: 0.2201\n",
      "Epoch 47/100\n",
      "235/235 [==============================] - 117s 497ms/step - loss: 1.6289 - accuracy: 0.5020 - val_loss: 3.1184 - val_accuracy: 0.2491\n",
      "Epoch 48/100\n",
      "235/235 [==============================] - 117s 497ms/step - loss: 1.6039 - accuracy: 0.5137 - val_loss: 3.1947 - val_accuracy: 0.2260\n",
      "Epoch 49/100\n",
      "235/235 [==============================] - 118s 501ms/step - loss: 1.6216 - accuracy: 0.5051 - val_loss: 3.0875 - val_accuracy: 0.2341\n",
      "Epoch 50/100\n",
      "161/235 [===================>..........] - ETA: 29s - loss: 1.5871 - accuracy: 0.5148"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c98180-2330-43f8-b419-7de5385afbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
